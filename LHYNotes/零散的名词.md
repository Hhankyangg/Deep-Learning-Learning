**正则化**

https://zh.wikipedia.org/zh-hans/%E6%AD%A3%E5%88%99%E5%8C%96_(%E6%95%B0%E5%AD%A6)

**`Weight Decay` - L2正则项**

we multiply the sum of squares with another smaller number. This number is called weight decay or wd.

Our loss function now looks as follows:
`Loss = MSE(y_hat, y) + wd * sum(w^2)`

减小模型复杂度（弹性），降低过拟合的概率

**残差（residual）和残差网络**

https://www.cnblogs.com/wuliytTaotao/p/9560205.html

**CNN 输出大小公式**

$W$ 为输入大小，$F$ 为卷积核大小，$P$ 为填充大小（Padding），$S$ 为步长（Stride），$N$ 为输出大小，则有：

$$
N = \frac {W - F + 2P} {S} + 1
$$