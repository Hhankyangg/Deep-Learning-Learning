### 正则化

https://zh.wikipedia.org/zh-hans/%E6%AD%A3%E5%88%99%E5%8C%96_(%E6%95%B0%E5%AD%A6)

**`Weight Decay` - L2正则项**

we multiply the sum of squares with another smaller number. This number is called weight decay or wd.

Our loss function now looks as follows:
`Loss = MSE(y_hat, y) + wd * sum(w^2)`

减小模型复杂度（弹性），降低过拟合的概率

### 残差（residual）和残差网络

https://www.cnblogs.com/wuliytTaotao/p/9560205.html

### CNN 输出大小公式

$W$ 为输入大小，$F$ 为卷积核大小，$P$ 为填充大小（Padding），$S$ 为步长（Stride），$N$ 为输出大小，则有：

$$
N = \frac {W - F + 2P} {S} + 1
$$

### 熵（Entropy）；KL散度（KL Divergence）等

#### 熵

若一个离散随机变量 $\mathbf{X}$ 的可能取值为 $X=\{x_1,x_2,\cdots,x_n\}$，而对应的概率为 $p_i = p(X = x_i)$，则随机变量 $\mathbf{X}$ 的熵定义为：

$$
H(X) = - \sum_{i=1}^{n} p(x_i) \log p(x_{i})
$$

#### 交叉熵

针对上述离散变量的概率分布 $p(x)、q(x)$ 而言，其交叉熵定义为：

$$
H(p,q) = - \sum_{x} p(x) \log q(x)
$$

> 在信息论中，交叉熵可认为是对预测分布 $q(x)$ 用真实分布 $p(x)$ 来进行编码时所需要的信息量大小。

#### KL散度（KL Divergence）

KL散度可以用来衡量两个分布之间的差异：

$$
\begin{aligned}
D_{KL}(p||q) = & H(p,q)-H(p) \\
=& - \sum_{x} p(x) \log q(x) - \sum_{x}-p(x)\log p(x)\\
=& -\sum_{x}p(x) \log \frac{q(x)}{p(x)}
\end{aligned}
$$

> 交叉熵损失函数实际上就是 KL散度，只不过因为分类问题的特殊性，其式与交叉熵相同：

在分类问题中，我们通过 `softmax` 函数得到一个分布，我们要通过 **KL散度** 衡量这个分布和目标分布（一个 one-hot 向量）的差异性：

$$
D_{KL}(p||q) = H(p,q)-H(p)
$$

我们易知 $H(p) = 0$；且对于真实分布（一个 one-hot 向量）只有概率为 1 的一项是非 0 的，所以有以下化简：

$$
\begin{aligned}
D_{KL}(p||q) = & H(p,q)\\
=& - \sum_{x} p(x) \log q(x)\\
=& -\log \text{softmax} \ q(x_i)\\
=& -\log \frac{\exp (x_i)}{\sum_j \exp (x_j)}
\end{aligned}
$$

**KL散度的数学性质**

- 正定性
$$
D_{KL}(p||q) \ge 0
$$
- 不对称性
KL散度并不是一个真正的度量或者距离，因为它不具有对称性：
$$
D(p \| q) \ne D(q \| p)
$$

#### JS散度（JS Divergence）

JS散度 度量了两个概率分布的相似度，基于 KL散度 的变体，解决了 KL散度 非对称的问题。一般地，JS散度 是对称的，其取值是 0 到 1 之间。

$$
JSD(P||Q)=\frac{1}{2}D_{KL}(P||\frac{P+Q}{2})+\frac{1}{2}D_{KL}(Q||\frac{P+Q}{2})
$$

> 对于 JS散度，当两个分布完全不重合时，JS散度始终为 1（$\log 2$）
